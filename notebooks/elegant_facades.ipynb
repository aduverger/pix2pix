{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSCJiIJ-IMJj"
   },
   "source": [
    "# ele.gan.t facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju1ZpbB4u2FJ"
   },
   "source": [
    "This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) using the pix2pix python library developed through our end of study project at Le Wagon bootcamp.\n",
    "\n",
    "**Authors** : Amor Hamza, Chaigneau Colin, Duverger Alexandre, Sadaouni Oumnia\n",
    "\n",
    "<p align=\"center\">\n",
    "<br>\n",
    "  <img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff80adc2b-9f33-4f35-8030-0eacdb2e2a77%2Ftest2.jpg?table=block&id=36acfc6b-6b78-4722-8905-2486c407cfb6&spaceId=b9592099-2b37-4101-aeaa-24da873f1526&width=2000&userId=46c52212-dcdf-44a2-b4b8-796d09177007&cache=v2\" />\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "Generated image from the cGan model\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK8Fp8CZB8bZ"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1LTrlvCtGad"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pip\n",
    "!pip install -q git+https://github.com/aduverger/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGbQLHfQsO2x"
   },
   "outputs": [],
   "source": [
    "from pix2pix.data import *\n",
    "from pix2pix.cgan import *\n",
    "from pix2pix.models import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dCq8hCdIo6_"
   },
   "source": [
    "## Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usmlkRGKxXGQ"
   },
   "source": [
    "First download the dataset from Berkeley and save it on this Colab temporary drive. You can also use your own dataset, as long as it meets the following criteria :\n",
    "- Use 3 folders : train, val, test\n",
    "- Input and output image must be on the same file, which has a shape of 512x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-U_IrFBzUZm"
   },
   "outputs": [],
   "source": [
    "!curl -O http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "!tar -xzf facades.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeR1KbrrPOkV"
   },
   "source": [
    "Create tensorflow datasets from the images you just downloaded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dihCz3W3vtzr"
   },
   "outputs": [],
   "source": [
    "ds_train, ds_val, ds_test = get_dataset(host='/content/facades')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQI_HGTvTtRS"
   },
   "source": [
    "## Create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9rkG74RLV6F"
   },
   "source": [
    "GANs rely on a generator that learns to generate new images, and a discriminator that learns to distinguish synthetic images from real images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WFu_7VbVBgc"
   },
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqJgtQ2jAb0T"
   },
   "source": [
    "#### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg30IYyyVF5s"
   },
   "source": [
    "The Encoder-Decoder generator uses tf.keras.layers.Conv2D (downsampling) and tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a image : encoder-decoder.\n",
    "\n",
    "We use strided convolutions instead of pooling layers, as describe in deep convolutional GAN original paper.\n",
    "All ReLU are leaky, with slope 0.2.\n",
    "Dropout with a rate of 50% are applied to the first 3 layers of the decoder.\n",
    "Weights are initialized from a Gaussian distribution with mean 0 and standard deviation 0.02.\n",
    "\n",
    "encoder architecture:\n",
    "C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\n",
    "decoder architecture:\n",
    "CD512-CD512-CD512-C512-C256-C128-C64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL7OnsAjgncR"
   },
   "source": [
    " ![encoder-decoder](./img/encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I-uIgku6G_-"
   },
   "source": [
    "Visualize the encoder model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lA448PciL6e"
   },
   "outputs": [],
   "source": [
    "encoder = make_generator_encoder_model()\n",
    "plot_model(encoder, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEDBfDfF6Q5u"
   },
   "source": [
    "Visualize the decoder model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1HUkS0ojkOG"
   },
   "outputs": [],
   "source": [
    "decoder = make_generator_decoder_model()\n",
    "plot_model(decoder, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkolhhY-6u1E"
   },
   "source": [
    "Visualize the encoder-decoder model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1CkUv7V6i3f"
   },
   "outputs": [],
   "source": [
    "encoder_decoder = make_generator_encoder_decoder_model(encoder, decoder)\n",
    "plot_model(encoder_decoder, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJJbB06swGsw"
   },
   "source": [
    "You can try to train an encoder-decoder alone, to see how well it performs at generating realistic building facades from sketches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr_9iSafwHUe"
   },
   "outputs": [],
   "source": [
    "encoder_decoder_alone = CGAN(encoder_decoder, cgan_mode=False)\n",
    "n_epoch = 200\n",
    "\n",
    "encoder_decoder_alone.fit(train_ds=ds_train,\n",
    "                          val_ds=ds_val,\n",
    "                          epochs=n_epoch,\n",
    "                          epoch_gen=n_epoch,\n",
    "                          l1_lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQTBGUppAMjr"
   },
   "source": [
    "#### U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4i0KyHX7Emm"
   },
   "source": [
    "If you tried to train the encoder-decoder, you should have noticed that it performs quite badly as a generator.\n",
    "\n",
    "A U-Net architecture actually performs better as it \"remembers\" the input images during its forward propagation.\n",
    "\n",
    "The U-Net architecture is identical to the Encoder-Decoder except with skip connections between each layer i in the encoder and layer n−i in the decoder, where n is the total number of layers. The skip connections concatenate activations from layer i to layer n − i. This changes the number of channels in the decoder:\n",
    "CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reRmOBGfg6JT"
   },
   "source": [
    " ![unet](./img/unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xIgcwRL96mm"
   },
   "source": [
    "Visualize the U-Net model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfJzC7Bs95AB"
   },
   "outputs": [],
   "source": [
    "unet = make_generator_unet_model()\n",
    "plot_model(unet, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hGaHXZH2tf3"
   },
   "source": [
    "You can try to train a U-net alone, to see how well it performs at generating realistic building facades from sketches :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsN94hHw2whr"
   },
   "outputs": [],
   "source": [
    "unet_alone = CGAN(unet, cgan_mode=False)\n",
    "n_epoch = 200\n",
    "\n",
    "unet_alone.fit(train_ds=ds_train,\n",
    "               val_ds=ds_val,\n",
    "               epochs=n_epoch,\n",
    "               epoch_gen=n_epoch,\n",
    "               l1_lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl2E3MR6aazy"
   },
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GujQzgTOai4R"
   },
   "source": [
    "As we noticed from using generators alone to performs our task, they don't do so well..\n",
    "\n",
    "We therefore need to introduce an additional neural network, called a discriminator, to challenge our generator to perform better.\n",
    "\n",
    "The discriminator is a CNN-based image classifier : C64-C128-C256-C512-C512-C512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTOJFT9chTkW"
   },
   "source": [
    " ![gan](./img/gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflE7QXm-FIA"
   },
   "source": [
    "Visualize the discriminator model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyK-LVJV-Imr"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PbHbGEP8T2c"
   },
   "source": [
    "Using this simple discriminator architecture performs quite well for our task, but there is a tricky concept that the research paper introduced to further improve the discriminator : convolutional “PatchGAN” classifier.\n",
    "\n",
    "It only penalizes structure at the scale of image patch. This discriminator tries to classify if each 70x70 patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSjNXv7ehrzN"
   },
   "source": [
    " ![patch-gan](./img/patchgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07xKeyDv-gns"
   },
   "source": [
    "Visualize the Patch-discriminator model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuG_4sLr-kBW"
   },
   "outputs": [],
   "source": [
    "patch_discriminator = make_patch_discriminator_model()\n",
    "plot_model(patch_discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P11CHNIBg32_"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcScVmrEDr6H"
   },
   "source": [
    "Instantiate a cGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58y-WLMW8aUq"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_unet_model()\n",
    "#discriminator = make_discriminator_model()\n",
    "discriminator = make_patch_discriminator_model()\n",
    "model = CGAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbhGOXOvD2x1"
   },
   "source": [
    "Little hack to always have the same images displayed during the training process. If you prefer to have random images at each epoch, don't run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeXxKpUEEBka"
   },
   "outputs": [],
   "source": [
    "model.random_sample=False\n",
    "model.paint_train, model.real_train = \\\n",
    "                        load_and_split_image('/content/facades/train/20.jpg')\n",
    "model.paint_val, model.real_val = \\\n",
    "                        load_and_split_image('/content/facades/val/21.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk5BFfhiEkQx"
   },
   "source": [
    "Use the cell below if you wan to change the optimizers of the generator and discriminator. It can be usefull if you want to lower their learning rates after you already trained your model for a certain number of epochs. \n",
    "\n",
    "**Don't run this cell if you start your training from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWo3MvTAMPl6"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.00005, beta_1=0.5)\n",
    "model.compile(gen_optimizer, disc_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyqbJhKGFG5z"
   },
   "source": [
    "Then finally, train your model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjqKlrja8vk9"
   },
   "outputs": [],
   "source": [
    "# Use `init` to start over your previous training.\n",
    "# e.g. : you trained your model for 150 epochs. Then set init=150 so that your model will start training at epoch=150\n",
    "init = 0\n",
    "n_epoch = 200\n",
    "model.fit(train_ds=ds_train, val_ds=ds_val,\n",
    "          epochs=init+n_epoch, initial_epoch=init,\n",
    "          epoch_gen=5, epoch_disc=0,\n",
    "          k=1, l1_lambda=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sedQ-OBHWnOW"
   },
   "source": [
    "## Test and save your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9FeszdYWPAs"
   },
   "source": [
    "### Save and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2Rk1fyEyhtT"
   },
   "source": [
    "Save model, history and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trmTu9lyWQL8"
   },
   "outputs": [],
   "source": [
    "model_name = 'my_model'\n",
    "model.generator.save(f'{model_name}/generator'+'.h5', save_format='h5')\n",
    "model.discriminator.save(f'{model_name}/discriminator'+'.h5', save_format='h5')\n",
    "hist_file = open(f'{model_name}/history.pkl', 'wb')\n",
    "pickle.dump(model.history, hist_file)\n",
    "hist_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_25IgPqHKFA"
   },
   "source": [
    "You can load your own model if you want. Here, we provide an already trained model, which can also be used from our website [YesWeGan](https://yeswegan.herokuapp.com/).\n",
    "Download the [trained model from our drive](https://drive.google.com/file/d/19YAi9xt5s4GbMjb6junyXHbukn6gKGoJ/view?usp=sharing) and put it at the root of this Colab Drive. Then load the model using the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2BrJjaXTAwL"
   },
   "outputs": [],
   "source": [
    "new_generator = load_model('elegant_facade_trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boC98UUdWu91"
   },
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4JG_q_KQfv7"
   },
   "outputs": [],
   "source": [
    "_, _, paint_ds_test, _, _, real_ds_test = get_facades_datasets(host='/content/facades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKZk8xoD5aOG"
   },
   "outputs": [],
   "source": [
    "paint_test = np.array([X for batch_X in iter(paint_ds_test) for X in batch_X])\n",
    "real_test = np.array([Y for batch_Y in iter(real_ds_test) for Y in batch_Y])\n",
    "fake_test = new_generator(paint_test, training=True)\n",
    "@interact(index = range(paint_test.shape[0]))\n",
    "def plot_pred(index):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(30,30))\n",
    "    axs[0].imshow((paint_test[index] * 127.5 + 127.5).astype('uint8'))\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow((fake_test[index] * 127.5 + 127.5).numpy().astype('uint8'))\n",
    "    axs[1].axis('off')\n",
    "    axs[2].imshow((real_test[index] * 127.5 + 127.5).astype('uint8'))\n",
    "    axs[2].axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NK8Fp8CZB8bZ",
    "9dCq8hCdIo6_",
    "XQI_HGTvTtRS",
    "_WFu_7VbVBgc",
    "eqJgtQ2jAb0T",
    "lQTBGUppAMjr",
    "Zl2E3MR6aazy",
    "P11CHNIBg32_",
    "sedQ-OBHWnOW",
    "s9FeszdYWPAs",
    "boC98UUdWu91"
   ],
   "name": "elegant_facades.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0530bae79fa5effdc380a6500ec199ad2a41423b17abf4b896a3b0dcfe1e45ee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('pix2pix': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
