{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elegant_facades.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NK8Fp8CZB8bZ",
        "9dCq8hCdIo6_",
        "XQI_HGTvTtRS",
        "_WFu_7VbVBgc",
        "eqJgtQ2jAb0T",
        "lQTBGUppAMjr",
        "Zl2E3MR6aazy",
        "P11CHNIBg32_",
        "sedQ-OBHWnOW",
        "s9FeszdYWPAs",
        "boC98UUdWu91"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSCJiIJ-IMJj"
      },
      "source": [
        "# ele.gan.t facades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju1ZpbB4u2FJ"
      },
      "source": [
        "This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) using the pix2pix python library developed through our end of study project at Le Wagon bootcamp.\n",
        "\n",
        "**Authors** : Amor Hamza, Chaigneau Colin, Duverger Alexandre, Sadaouni Oumnia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK8Fp8CZB8bZ"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1LTrlvCtGad"
      },
      "source": [
        "!pip install -q -U pip\n",
        "!pip install -q git+https://github.com/aduverger/pix2pix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGbQLHfQsO2x"
      },
      "source": [
        "from pix2pix.data import *\n",
        "from pix2pix.cgan import *\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from ipywidgets import interact\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dCq8hCdIo6_"
      },
      "source": [
        "## Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usmlkRGKxXGQ"
      },
      "source": [
        "First download the dataset from Berkeley and save it on this Colab temporary drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-U_IrFBzUZm"
      },
      "source": [
        "!curl -O http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
        "!tar -xzf facades.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeR1KbrrPOkV"
      },
      "source": [
        "Get the tf.datasets you need for training a cGan model on the facades dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dihCz3W3vtzr"
      },
      "source": [
        "ds_train, ds_val, ds_test = get_dataset(host='/content/facades')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQI_HGTvTtRS"
      },
      "source": [
        "## Create the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9rkG74RLV6F"
      },
      "source": [
        "GANs rely on a generator that learns to generate new images, and a discriminator that learns to distinguish synthetic images from real images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WFu_7VbVBgc"
      },
      "source": [
        "### The Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqJgtQ2jAb0T"
      },
      "source": [
        "#### Encoder-Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg30IYyyVF5s"
      },
      "source": [
        "The Encoder-Decoder generator uses tf.keras.layers.Conv2D (downsampling) and tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a image : encoder-decoder.\n",
        "\n",
        "We use strided convolutions instead of pooling layers, as describe in deep convolutional GAN original paper. All ReLU are leaky, with slope 0.2. Dropout with a rate of 50% are applied to the first 3 layers of the decoder.\n",
        "\n",
        "encoder architecture:\n",
        "C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\n",
        "decoder architecture:\n",
        "CD512-CD512-CD512-C512-C256-C128-C64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmOQrJQHTbDZ"
      },
      "source": [
        "def make_generator_encoder_model():\n",
        "    encoder = tf.keras.Sequential()\n",
        "\n",
        "    encoder.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=(256, 256, 3)))\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 128, 128, 64)\n",
        "\n",
        "    encoder.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 64, 64, 128)\n",
        "\n",
        "    encoder.add(layers.Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 32, 32, 256)\n",
        "\n",
        "    encoder.add(layers.Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 16, 16, 512)\n",
        "\n",
        "    encoder.add(layers.Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 8, 8, 512)\n",
        "\n",
        "    encoder.add(layers.Conv2D(512, (3,3), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 4, 4, 512)\n",
        "\n",
        "    encoder.add(layers.Conv2D(512, (2,2), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 2, 2, 512)\n",
        "\n",
        "    encoder.add(layers.Conv2D(512, (2,2), strides=(2,2), padding='same'))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.LeakyReLU(alpha=0.2))\n",
        "    assert encoder.output_shape == (None, 1, 1, 512)\n",
        "\n",
        "    encoder.add(layers.Flatten())\n",
        "    assert encoder.output_shape == (None, 512)\n",
        "\n",
        "    encoder.add(layers.Dense(512, activation='tanh'))\n",
        "\n",
        "    return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I-uIgku6G_-"
      },
      "source": [
        "Visualize the encoder model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lA448PciL6e"
      },
      "source": [
        "encoder = make_generator_encoder_model()\n",
        "tf.keras.utils.plot_model(encoder, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0fjW1lxdcJV"
      },
      "source": [
        "def make_generator_decoder_model():\n",
        "    decoder = tf.keras.Sequential()\n",
        "\n",
        "    decoder.add(layers.Dense(2*2*512, input_shape=(512,), activation='tanh'))\n",
        "\n",
        "    decoder.add(layers.Reshape((2,2,512)))\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(512, (2,2), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    encoder.add(layers.Dropout(0.5))\n",
        "    assert decoder.output_shape == (None, 4, 4, 512)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(512, (2,2), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    encoder.add(layers.Dropout(0.5))\n",
        "    assert decoder.output_shape == (None, 8, 8, 512)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(512, (3,3), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    encoder.add(layers.Dropout(0.5))\n",
        "    assert decoder.output_shape == (None, 16, 16, 512)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    assert decoder.output_shape == (None, 32, 32, 512)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    assert decoder.output_shape == (None, 64, 64, 256)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    assert decoder.output_shape == (None, 128, 128, 128)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same'))\n",
        "    decoder.add(layers.BatchNormalization())\n",
        "    decoder.add(layers.ReLU())\n",
        "    assert decoder.output_shape == (None, 256, 256, 64)\n",
        "\n",
        "    decoder.add(layers.Conv2DTranspose(3, (5,5), strides=(1,1), padding='same', activation='tanh'))\n",
        "\n",
        "    assert decoder.output_shape == (None, 256, 256, 3)\n",
        "\n",
        "    return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEDBfDfF6Q5u"
      },
      "source": [
        "Visualize the decoder model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1HUkS0ojkOG"
      },
      "source": [
        "decoder = make_generator_decoder_model()\n",
        "tf.keras.utils.plot_model(decoder, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9vQn_1sjJNY"
      },
      "source": [
        " def make_generator_encoder_decoder_model(encoder, decoder):\n",
        "    inp = layers.Input((256, 256, 3))\n",
        "    encoded = encoder(inp)\n",
        "    decoded = decoder(encoded)\n",
        "    encoder_decoder = tf.keras.Model(inp, decoded)\n",
        "    return encoder_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkolhhY-6u1E"
      },
      "source": [
        "Visualize the encoder-decoder model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1CkUv7V6i3f"
      },
      "source": [
        "encoder_decoder = make_generator_encoder_decoder_model(encoder, decoder)\n",
        "tf.keras.utils.plot_model(encoder_decoder, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQTBGUppAMjr"
      },
      "source": [
        "#### U-net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4i0KyHX7Emm"
      },
      "source": [
        "A U-Net architecture actually performs better as a generator than a Encoder-Decoder.\n",
        "\n",
        "The U-Net architecture is identical to the Encoder-Decoder except with skip connections between each layer i in the encoder and layer n−i in the decoder, where n is the total number of layers. The skip connections concatenate activations from layer i to layer n − i. This changes the number of channels in the decoder:\n",
        "CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
        "\n",
        "Weights are initialized from a Gaussian distribution with mean 0 and standard deviation 0.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWXJPrD0_5co"
      },
      "source": [
        "def block_conv2D(filters, kernel_size=4, strides=(2,2), with_batch_norm=True, relu_alpha=0.3):\n",
        "    '''\n",
        "        Return a block of layers consisting of Conv2D-BatchNormal-LeakyReLU or Conv2D-LeakyReLU layers\n",
        "    '''\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(layers.Conv2D(filters, kernel_size, strides=strides, padding='same', kernel_initializer=initializer, use_bias=False))\n",
        "    if with_batch_norm:\n",
        "        block.add(layers.BatchNormalization())\n",
        "    block.add(layers.LeakyReLU(alpha=relu_alpha))\n",
        "\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6N1g_iFATLc"
      },
      "source": [
        "def block_conv2D_transpose(filters, kernel_size=4, strides=(2,2), with_dropout=False, dropout_rate=0.5):\n",
        "    '''\n",
        "        Return a block of layers consisting of Conv2D-BatchNorm-Dropout-ReLU or Conv2D-BatchNorme-ReLU\n",
        "    '''\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
        "    block = tf.keras.Sequential()\n",
        "    block.add(layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
        "    block.add(layers.BatchNormalization())\n",
        "    if with_dropout:\n",
        "        block.add(layers.Dropout(dropout_rate))\n",
        "    block.add(layers.ReLU())\n",
        "\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93cXkt9AYST"
      },
      "source": [
        "def make_generator_unet_model():\n",
        "\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
        "\n",
        "    inputs = layers.Input(shape=(256, 256, 3))\n",
        "    res = []\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = block_conv2D(64, with_batch_norm=False)(inputs)\n",
        "    # Shape = (128, 128, 64)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(128)(x)\n",
        "    # Shape = (64, 64, 128)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(256)(x)\n",
        "    # Shape = (32, 32, 256)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(512)(x)\n",
        "    # Shape = (16, 16, 512)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(512)(x)\n",
        "    # Shape = (8, 8, 512)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(512)(x)\n",
        "    # Shape = (4, 4, 512)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(512)(x)\n",
        "    # Shape = (2, 2, 512)\n",
        "    res.append(x)\n",
        "\n",
        "    x = block_conv2D(512, with_batch_norm=False)(x)\n",
        "    # Shape = (1, 1, 512) --> LATENT SPACE\n",
        "\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    x = block_conv2D_transpose(512, with_dropout=True)(x)\n",
        "    x = layers.Concatenate()([x, res[-1]])\n",
        "    # Shape = (2, 2, 1024)\n",
        "\n",
        "    x = block_conv2D_transpose(512, with_dropout=True)(x)\n",
        "    x = layers.Concatenate()([x, res[-2]])\n",
        "    # Shape = (4, 4, 1024)\n",
        "\n",
        "    x = block_conv2D_transpose(512, with_dropout=True)(x)\n",
        "    x = layers.Concatenate()([x, res[-3]])\n",
        "    # Shape = (8, 8, 1024)\n",
        "\n",
        "    x = block_conv2D_transpose(512)(x)\n",
        "    x = layers.Concatenate()([x, res[-4]])\n",
        "    # Shape = (16, 16, 1024)\n",
        "\n",
        "    x = block_conv2D_transpose(256)(x)\n",
        "    x = layers.Concatenate()([x, res[-5]])\n",
        "    # Shape = (32, 32, 512)\n",
        "\n",
        "    x = block_conv2D_transpose(128)(x)\n",
        "    x = layers.Concatenate()([x, res[-6]])\n",
        "    # Shape = (64, 64, 256)\n",
        "\n",
        "    x = block_conv2D_transpose(64)(x)\n",
        "    x = layers.Concatenate()([x, res[-7]])\n",
        "    # Shape = (128, 128, 128)\n",
        "\n",
        "    # Last ouput \n",
        "    outputs = layers.Conv2DTranspose(3, kernel_size=4, strides=(2,2), activation=\"tanh\", kernel_initializer=initializer, padding=\"same\")(x)\n",
        "    # Shape = (256, 256, 3)\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xIgcwRL96mm"
      },
      "source": [
        "Visualize the U-Net model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfJzC7Bs95AB"
      },
      "source": [
        "unet = make_generator_unet_model()\n",
        "tf.keras.utils.plot_model(unet, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl2E3MR6aazy"
      },
      "source": [
        "### The Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GujQzgTOai4R"
      },
      "source": [
        "The discriminator is a CNN-based image classifier : C64-C128-C256-C512-C512-C512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-uPS0VMbMeW"
      },
      "source": [
        "def make_discriminator_model(alpha=0.3, cgan_mode=False):\n",
        "\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02, seed=None)\n",
        "\n",
        "    discriminator = tf.keras.Sequential()\n",
        "    if cgan_mode:\n",
        "        discriminator.add(layers.Conv2D(64, 4, strides=(2,2), input_shape=(256, 256, 6), kernel_initializer=initializer, padding='same'))\n",
        "    else:\n",
        "        discriminator.add(layers.Conv2D(64, 4, strides=(2,2), input_shape=(256, 256, 3), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (128, 128, 64)\n",
        "\n",
        "    discriminator.add(layers.Conv2D(128, 4, strides=(2,2), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.BatchNormalization())\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (64, 64, 128)\n",
        "\n",
        "    discriminator.add(layers.Conv2D(256, 4, strides=(2,2), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.BatchNormalization())\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (32, 32, 256)\n",
        "\n",
        "    discriminator.add(layers.Conv2D(512, 4, strides=(2,2), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.BatchNormalization())\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (16, 16, 512)\n",
        "\n",
        "    discriminator.add(layers.Conv2D(512, 4, strides=(2,2), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.BatchNormalization())\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (8, 8, 512)\n",
        "\n",
        "    discriminator.add(layers.Conv2D(512, 4, strides=(2,2), kernel_initializer=initializer, padding='same'))\n",
        "    discriminator.add(layers.BatchNormalization())\n",
        "    discriminator.add(layers.LeakyReLU(alpha=alpha))\n",
        "    # Shape = (4, 4, 512)\n",
        "\n",
        "    discriminator.add(layers.Flatten())\n",
        "\n",
        "    discriminator.add(layers.Dense(1, kernel_initializer=initializer))\n",
        "\n",
        "    return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nflE7QXm-FIA"
      },
      "source": [
        "Visualize the discriminator model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyK-LVJV-Imr"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PbHbGEP8T2c"
      },
      "source": [
        "The research paper introcuded the concept of convolutional “PatchGAN” classifier, which only penalizes structure at the scale of image patch. This discriminator tries to classify if each 70x70 patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of the Discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vyhqryJ2FQT"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True, alpha=0.3):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU(alpha=alpha))\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtsqHxCHoVu6"
      },
      "source": [
        "def make_patch_discriminator_model(cgan_mode=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    if cgan_mode:\n",
        "        inp = tf.keras.layers.Input(shape=[256, 256, 6])\n",
        "    else:\n",
        "        inp = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "    down1 = downsample(64, 4, False)(inp)  # (batch_size, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07xKeyDv-gns"
      },
      "source": [
        "Visualize the Patch-discriminator model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuG_4sLr-kBW"
      },
      "source": [
        "patch_discriminator = make_patch_discriminator_model()\n",
        "tf.keras.utils.plot_model(patch_discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11CHNIBg32_"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcScVmrEDr6H"
      },
      "source": [
        "Instantiate a cGAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58y-WLMW8aUq"
      },
      "source": [
        "generator = make_generator_unet_model()\n",
        "#discriminator = make_discriminator_model(cgan_mode=True)\n",
        "discriminator = make_patch_discriminator_model(cgan_mode=True)\n",
        "model = CGAN(generator, discriminator, cgan_mode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbhGOXOvD2x1"
      },
      "source": [
        "Little hack to always have the same images displayed during the training process. If you prefer to have random images at each epoch, don't run this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeXxKpUEEBka"
      },
      "source": [
        "model.random_sample=False\n",
        "model.paint_train, model.real_train = \\\n",
        "                        load_and_split_image('/content/facades/train/20.jpg')\n",
        "model.paint_val, model.real_val = \\\n",
        "                        load_and_split_image('/content/facades/val/21.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk5BFfhiEkQx"
      },
      "source": [
        "Use the cell below if you wan't to change the optimizers of the generator and discriminator. It can be usefull if you want to lower their learning rates after you already trained your model for a certain number of epochs. \n",
        "\n",
        "**Don't run this cell if you start your training from scratch.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWo3MvTAMPl6"
      },
      "source": [
        "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.5)\n",
        "model.compile(gen_optimizer, disc_optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqbJhKGFG5z"
      },
      "source": [
        "Then finally, train your model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjqKlrja8vk9"
      },
      "source": [
        "# Use `init` to start over your previous training.\n",
        "# e.g. : you trained your model for 150 epochs. Then set init=150 so that your model will start training at epoch=150\n",
        "init = 0 \n",
        "n_epoch = 200\n",
        "model.fit(train_ds=ds_train, val_ds=ds_val,\n",
        "          epochs=init+n_epoch, initial_epoch=init,\n",
        "          epoch_gen=5, epoch_disc=0,\n",
        "          k=1, l1_lambda=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sedQ-OBHWnOW"
      },
      "source": [
        "## Test and save your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9FeszdYWPAs"
      },
      "source": [
        "### Save and load models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Rk1fyEyhtT"
      },
      "source": [
        "Save model, history and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trmTu9lyWQL8"
      },
      "source": [
        "model_name = 'my_model'\n",
        "model.generator.save(f'{model_name}/generator'+'.h5', save_format='h5')\n",
        "model.discriminator.save(f'{model_name}/discriminator'+'.h5', save_format='h5')\n",
        "hist_file = open(f'{model_name}/history.pkl', 'wb')\n",
        "pickle.dump(model.history, hist_file)\n",
        "hist_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_25IgPqHKFA"
      },
      "source": [
        "You can load your own model if you want. Here, we provide an already trained model, which can also be used from our website [YesWeGan](https://yeswegan.herokuapp.com/).\n",
        "Download the [trained model from our drive](https://drive.google.com/file/d/19YAi9xt5s4GbMjb6junyXHbukn6gKGoJ/view?usp=sharing) and put it at the root of this Colab Drive. Then load the model using the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2BrJjaXTAwL"
      },
      "source": [
        "new_generator = tf.keras.models.load_model('elegant_facade_trained_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boC98UUdWu91"
      },
      "source": [
        "### Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4JG_q_KQfv7"
      },
      "source": [
        "_, _, paint_ds_test, _, _, real_ds_test = get_facades_datasets(host='/content/facades')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZk8xoD5aOG"
      },
      "source": [
        "paint_test = np.array([X for batch_X in iter(paint_ds_test) for X in batch_X])\n",
        "real_test = np.array([Y for batch_Y in iter(real_ds_test) for Y in batch_Y])\n",
        "fake_test = new_generator(paint_test, training=True)\n",
        "@interact(index = range(paint_test.shape[0]))\n",
        "def plot_pred(index):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(30,30))\n",
        "    axs[0].imshow((paint_test[index] * 127.5 + 127.5).astype('uint8'))\n",
        "    axs[0].axis('off')\n",
        "    axs[1].imshow((fake_test[index] * 127.5 + 127.5).numpy().astype('uint8'))\n",
        "    axs[1].axis('off')\n",
        "    axs[2].imshow((real_test[index] * 127.5 + 127.5).astype('uint8'))\n",
        "    axs[2].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}